{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo usado para la demo\n",
    "### Adrián Arnaiz\n",
    "Hemos elegido para la demo el modelo **MLP con 10 capas ocultas, que funcionará (entrenado) con las características tipo embeddings extraídas de los audios tipo read_text mediante VGGish.** \n",
    "\n",
    "Hemos elegido este modelo por varias razones:\n",
    "* Es el que **mejor funciona sin dividir por sexos** (posible mejora futura: derivar a diferentes modelos según el sexo del paciente)\n",
    "* Aporta novedad al estado del arte: Saca las **caracteristicas mediante VGGish** (red neuronal con capas convolucionales) y **predice mediante MLP**, el cual es otra red neuronal, un perceptrón multicapa (Multi Layer Perceptron).\n",
    "* La extracción de características mediante **VGGish** es considerablemente más rapida que mediante la librería Disvoice. En contra, las características que saca VGGish no sabemos que son, mediante que las de Disvoice sí que son interpretables.\n",
    "* Los audios tipo **read_text**, como ya hemos visto en el estado del arte, nos dan más información acerca de la enfermedad. Sacar conclusiones de una vocal sostenida puede ser ventajista, tal y como hemos visto y está explicado en la documentación.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation 10 Fold\n",
    "Haremos validación cruzada para ver los resultados del modelo con esta técnica. Sin embargo, para guardar luego el modelo, deberemos hacer un fit normal ya que con cross_val_score no se nos permite guardar un modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cargaDatosVggishEmbeddings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import SCORERS, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = cargaDatosVggishEmbeddings\n",
    "read_data = loader.load_vggish_embed_rt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 258), (100,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=read_data.data\n",
    "y=read_data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('norm', MinMaxScaler()), ('clf', MLPClassifier(hidden_layer_sizes=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72, 0.88, 0.88, 0.96, 0.88, 1.  , 1.  , 1.  , 0.76, 0.84])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = cross_val_score(pipe,X,y,cv=10,scoring='roc_auc')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Guardamos el modelo\n",
    "Hacemos la validación cruzada a mano, y guardamos uno de los modelos realizados en un sprint concreto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7777777777777778,\n",
       "  0.9166666666666667,\n",
       "  0.7083333333333334,\n",
       "  0.9166666666666667,\n",
       "  0.7777777777777778,\n",
       "  0.9166666666666667,\n",
       "  0.9166666666666667,\n",
       "  1.0,\n",
       "  0.7777777777777778,\n",
       "  0.8],\n",
       " 0.8508333333333334)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelos = dict()\n",
    "i=0\n",
    "aucs = []\n",
    "for train_index, test_index in fold.split(X, y):\n",
    "    pipe = Pipeline([('norm', MinMaxScaler()), ('clf', MLPClassifier(hidden_layer_sizes=10))])\n",
    "    #Dividimos por splits\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #Entrenamos\n",
    "    mod = pipe.fit(X_train, y_train)\n",
    "    #Predecimos\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    #Sacamos medida auc\n",
    "    auc_score = roc_auc_score(y_pred, y_test)\n",
    "    aucs.append(auc_score)\n",
    "    #guardamos el modelo creado con este split\n",
    "    modelos[i] = mod\n",
    "    i+=1\n",
    "aucs, np.mean(aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos el modelo con auc = 0.916. No elegimos el modelo con auc=1 ya que comprendemos que sobhreajusta. Un modelo con 100% de aciertop tampoco es fiable ya que entendemos que se sobreajusta a los datos. Por ello elegimos el modelo con auc 0.916, generaliza bien sin sobreajustar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = modelos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MLP10_vggish_rt.sav'\n",
    "#guardamos\n",
    "pickle.dump(my_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
