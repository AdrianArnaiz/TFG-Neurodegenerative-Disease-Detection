\apendice{Especificación de diseño}

\section{Introducción}
En éste apéndice, se describirán cómo están implementados los datos de esta aplicación, cuales son los procedimientos más relevantes de la aplicación, y cómo se organizan los proyectos en paquetes.

\section{Diseño de datos}
En este sección, explicaremos como están organizados tanto el conjunto de audios obtenido, cómo son y como se organizan las características extraídas de los audios, y el diagrama de clases de la aplicación.

\subsection{Conjunto de audios}
El conjunto de audios es el descrito en \cite{OrzCorpus}. También lo hemos descrito en la sección 5.3 de la memoria principal. Estos audios no se encuentran en el repositorio, ya que son de carácter privado. En \cite{OrzCorpus} se describe como: ``(...)incluye grabaciones de voz de 50 personas con PD y 50 controles sanos, 25 hombres y 25 mujeres en cada grupo. Todos los participantes son hablantes nativos de español colombiano. La edad de los hombres con PD varía de 33 a 77 años (media 62.2 $\pm$ 11.2), la edad de las mujeres con PD varía de 44 a 75 años (media 60.1 $\pm$ 7.8). Para el caso de los controles sanos, la edad de los hombres varía de 31 a 86 (media 61.2 $\pm$ 11.3) y la edad de las mujeres de 43 a 76 años (media 60.7 $\pm$ 7.7). Por lo tanto, la base de datos está bien equilibrada en términos de edad y género. Las grabaciones fueron capturadas en condiciones de control de ruido, en una cabina de prueba de sonido que se construyó en la Clínica Noel, en Medellín, Colombia. Los registros se muestrearon a 44100 Hz con 16 bits de resolución, utilizando un micrófono omnidireccional dinámico (Shure, SM 63L) que se usa comúnmente para aplicaciones profesionales. Las grabaciones se capturaron utilizando una tarjeta de audio profesional de hasta 24 bits y de tal forma que admite hasta 96 Kbps de frecuencia de muestreo (M-Audio, Fast Track C400). Estos audios están en formato wav.'' (p.343). Tendremos 100 audios de una frase y 100 audios de cada una de las 5 palabras. En caso de las vocales, tenemos 300 audios para cada vocal, en vez de 100, ya que hay 3 audios para cada vocal de cada persona. Aunque en el total hacen un número de audios de 2100 audios, como hemos comentado en la memoria, no se puede entrenar con todos a la vez. Tenemos que entrenar los modelos con las características extraídas de cada grupo de 100 o 300 audios (si son vocales) por separado. Esto hace que entrenemos los modelos con bases de datos pequeñas y corran riesgo de sobreajustarse.

Destacar también, que tenemos un archivo tipo excel donde se indica: edad,sexo, UPDRS, HY y tiempo desde la detección de la enfermedad del paciente. Este archivo se encuentra en \texttt{doc/masRecursos/PCGITA\_metadata.xlsx}.

\subsection{Características extraídas}
Como se ha comentado el la memoria hemos extraído diferentes características de los audios. Todas, se encuentran el el directorio o subdirectorios de \texttt{src/CaracterísticasExtraídas}. También, todas ellas están guardadas de manera serializada en formato tipo \english{numpy} con la herramienta \english{pickle}. Las características extraídas son las siguientes

\begin{itemize}
\item \textbf{Características Disvoice (primera fase):} se encuentran en \texttt{src/ CaracterísticasExtraídas} y se extraen en el \textit{notebook} \texttt{Extracción de características.ipynb} del directorio \texttt{src}. Están descritas más en detalle en la memoria en la sección 5.5.2 (Aspectos Relevantes - Primera Fase: atributos Disvoice - Modelado del discurso). A grandes rasgos, de los audios de la frase sacamos medidas de fonación, articulación y prosodia; de los audios de cada palabra sacamos medidas de fonación y articulación, y de los audios de cada vocal sacamos únicamente medidas de fonación. Hay un total de \textbf{18 conjuntos} diferentes de características. El tamaño de las matrices de características es el siguiente
	\begin{itemize}
	\item Fonación: 100$\times$30 y 300$\times$30 (para vocales). Se sacan 29 medidas más la clase. 
	\item Articulación: 100$\times$489. Se sacan 488 medidas más la clase.
	\item Prosodia: 100$\times$39. Se sacan 28 medidas más la clase.
	\end{itemize}

\item \textbf{Características MODIFICADAS Disvoice (segunda fase):} se encuentran en \texttt{src/ CaracterísticasExtraídas/EdadYSexo} y en \texttt{src/CaracterísticasExtraídas/DivisionSexo} y se extraen en \texttt{src/ Extracción de características MODIFICADAS Disvoice.ipynb}. Están descritas más en detalle en la memoria en la sección 5.6.1 (Aspectos Relevantes - Segunda Fase - Modelado del discurso). A grandes rasgos, primero se ha añadido los atributos edad y sexo a las anteriores características, por lo que tendremos otros 18 conjuntos de datos nuevos nuevos, cuya medida de las matrices es igual que la primera fase, pero teniendo 2 columnas más (2 características más). Por otro lado, se han dividido entre hombres y mujeres, por lo que tendremos otros 35 conjuntos de datos más, cada uno con la mitad de instancias que el original (tamaño: tendrán la mitad de filas que los de la fase 1, y tendrán 1 columna más: la edad). Por último, tenemos los conjuntos de características exactos a los de Orozco, solo los MFCC. Estos también están divididos por sexo. De estos últimos tenemos 24 conjuntos, cuyo tamaño es 50$\times$35 (34 medidas más la clase). Por lo hque forman un total de \textbf{78 conjuntos} de características diferentes.

\item \textbf{Características VGGish (tercera fase):} se encuentran en:
\begin{itemize}
\item \texttt{src/CaracterísticasExtraídas/vggish/embeddings}
\item \texttt{src/CaracterísticasExtraídas/vggish/espectros}.
\end{itemize}  
Están descritas más en detalle en la memoria en la sección 5.7.1 (Aspectos Relevantes - Tercera Fase - Modelado del discurso), y en los \textit{notebooks}. Se extraen para los tipos de audio de las 5 vocales y la frase los \textit{embeddings}, que genera unas matrices de características de tamaño 100$\times$256 para la frase y 300$\times$256 para cada vocal. Esto es debido a que \textit{VGGish} nos saca 128 \textit{embeddings} para cada fragmento de 25ms de un audio, y nosotros, hemos decidido que para formar un único array de cada audio, hacer la media y desviación de los \textit{embeddings}. Por otro lado, sacamos también los espectros de frecuencia del audio, con la herramienta del pre-procesado de VGGish. Los detalles se encuentran en la memoria, en el apartado recientemente indicado. El tamaño de las matrices de datos de los espectros es de 100$\times$128 para la frase y 300$\times$128 para cada vocal. Al igual que antes, es debido a que VGGish nos da 64 espectros de cada fragmento de 25ms, y hemos hecho la media y la desviación para tener un único array.
	
\end{itemize} 

\subsection{Diagrama de clases de la aplicación}
En este apartado se comentara las clases realizadas por nosotros en el proyecto. Es decir, las clases que se han creado para ayudar a los experimentos y realizar la aplicación. Señalamos que, únicamente se analizaran las realizadas por nosotros, todas la estructura de \textit{VGGish} no será analizada. De echo, por ejemplo, \textit{VGGish2Keras} es un conjunto de módulos con funciones, que no contienen ninguna clase.

\subsubsection{Clases de la etapa de experimentación}
 Cabe destacar que, en los experimentos con los clasificadores, se ha utilizado los \textit{notebooks} de \textit{Jupyter} para su realización. Por ello, en la etapa de realización de experimentos únicamente se realizaron 3 clases, sin ninguna relación entre ellas, las cuales únicamente se usaban en los \textit{notebooks} tanto de extracción de características como en los experimentos con clasificadores. 

De estas clases no se hará diagrama, ya que son clases independientes, y se explicarán en el apartado Manual del programador, ver capítulo \ref{sec:manualprog}. De estas 3 clases, 2 se alojan en el directorio \texttt{src} y son: \texttt{extractorCcas.py} y {experimenter.py}. Las otra está en el directorio \texttt{src/vggish} y se corresponde con el nombre \texttt{extractor\_ccas\_vggish.py}. También tenemos diferentes módulos de carga de datos, estos no son clases, siguen la estructura tipo los cargadores de datos de \textit{Scikit-Learn} (i.e \myurl{https://github.com/scikit-learn/scikit-learn/blob/7813f7efb/sklearn/datasets/base.py\#L326}{load\_iris}), funciones dentro de módulos que devuelven objetos de tipo \textit{Bunch}

\subsubsection{Clases de la aplicación}\label{sec:clases}
Para la aplicación se han creado las siguientes clases, alojadas en \texttt{src/demo}:
\begin{itemize}
\item \texttt{VentanaInicio} dentro de \texttt{main.py}: Clase que contiene toda la estructura gráfica de la ventada de la aplicación. Como suele pasar con los objetos de \textit{tkinter}, no tiene funciones, únicamente se inicializan todos los elementos gráficos (\textit{frames}, botones...), las variables necesarias para cada uno, y los mediadores. En esta clase, todos los elementos gráficos se guardarán como atributos (botones, \textit{frames}...) para un acceso más fácil a ellos y, por tanto, una modificación más sencilla.
\item \texttt{MediadorVentana} dentro de \texttt{MediadorVentana.py}: Clase que tiene como único atributo la ventana anterior. Contiene todos los métodos necesarios para la reproducción de los audios y muestra de detalles de la carga y reproducción de los mismos. \textbf{Encapsula la comunicación de los objetos gráficos de \texttt{VentanaInicio}}, en lo relativo a reproducción y carga de archivos. Debido a que comunica elementos, y los modifica, la mayoría de funciones no devuelve nada.
\item \texttt{MediadorPrediccion} dentro de \texttt{MediadorPrediccion.py}: Tiene dos atributos, la ventana de la aplicación y un objeto tipo \textit{FachadaPrediccion}. Las funciones de este mediador llaman a los métodos para predecir y mostrar gráficos de la fachada y se encarga de cambiar los elementos de la ventana. \textbf{Encapsula la comunicación de los objetos gráficos de \texttt{VentanaInicio}}, en lo relativo a la predicción y muestra de gráficos.
\item \texttt{FachadaPrediccion} dentro del fichero \texttt{FachadaPrediccion.py} del paquete predicción: Tiene dos atributos, el modelo \textit{keras VGGish} para la extracción de características de los audios y el modelo \textit{MLP} con 10 neuronas para la predicción. Se encarga de interactuar con los módulos de \textit{VGGish2Keras} para devolver los resultados de la predicción al mediador.
\item Módulos propios de \textit{VGGish} (\textit{VGGish2Keras}): \texttt{mel\_features.py, vggish\_keras.py, vggish\_params.py}. No son clases, son módulos Python que únicamente contienen métodos.
\end{itemize}
El diagrama de clases se puede ver en la Figura \ref{fig:DiagramaClasesPDD}. 

\imagen{DiagramaClasesPDD}{Diagrama de clases de la aplicación}


\section{Diseño procedimental}
En esta sección se mostrarán los diagramas de secuencia respectivos a 2 tareas principales de la aplicación: cargar audios (ver Figura \ref{fig:DS-cargar})y predecir de audios-muestra de gráficos (se ejecutan a la vez, cuando predices un audio, automáticamente se muestra sus gráficas, ver Figura \ref{fig:DS-prediccion}).
\subsection{Diagramas de secuencia}

\imagen{DS-cargar}{Diagrama de secuencia para cargar un audio}
\imagen{DS-prediccion}{Diagrama de secuencia para predicción y muestra de gráficos}


\section{Diseño arquitectónico}
En este apartado se definirá como está \textbf{estructurado el código en paquetes}. Únicamente describiremos los paquetes que contienen código, los demás directorios y paquetes están definidos en la sección \ref{sec:estructura}. Adicionalmente, vamos a definir el diseño arquitectónico de la aplicación, la cual sigue los patrones de diseño Mediador y Fachada.

\subsection{Patrón Mediador}
El patrón de diseño Mediador es un patrón que encapsula la comunicación entre diferentes elementos del programa, normalmente en tiempo de ejecución. \cite{wiki:mediador} Es nuestro caso, se ha utilizado para encapsular la comunicación entre diferentes elementos de la ventana de inicio. Es decir, encapsular la comunicación entre los diferentes \textit{frames}, para que la clase \texttt{VentanaInicio} se ocupara únicamente de definir los elementos gráficos. Se han utilizado dos mediadores, uno que encapsula la comunicación entre elementos derivada de la carga y reproducción de datos (\texttt{MediadorVentana}) y otro que encapsula la comunicación entre elementos derivada de las tareas de predicción (\texttt{MediadorPrediccion}).

\subsection{Patrón Fachada}
El patrón \textit{Facade}, o Fachada, proporciona una interfaz simple para un sistema de comportamiento más complejo. Este patrón reconoce que elementos del subsistema son los encargados de realizar una determinada labor, y los elementos por encima de la fachada únicamente se comunican con la fachada, no con todos los elementos del subsistema. \cite{wiki:fachada}. En nuestro caso ha sido usado para encapsular el funcionamiento de los módulos \textit{VGGish}. En un diagrama de clases de un patrón de fachada estándar, la fachada se comunicaría con las clases del subsistema. En nuestro caso, no se aprecia en el diagrama de clases la comunicación con el subsistema \textit{VGGish} ya que, como hemos dicho antes, son una serie de módulos que contienen funciones y, por tanto, no son clases. Nuestra \texttt{FachadaPrediccion} se comunicará con los módulos: \texttt{mel\_features.py, vggish\_keras.py, vggish\_params.py}.

\subsection{Paquetes del proyecto}
Paquetes del proyecto (ver Figura \ref{fig:diag_paquetes}):
\begin{itemize}
\item \textbf{src}: contiene todo el código del proyecto. En este paquete están los \textit{notebooks} de extracción de características \textit{Disvoice}, la clase extractora de características de \textit{Disvoice}, los módulos de carga de datos, las clase del experimentador, los 6 \textit{notebooks} de experimentos y  las pruebas de los cargadores de datos.
	\begin{itemize}
	\item \textbf{demo}: contiene el código de la aplicación del proyecto. En este directorio están las clases de la ventana de inicio, los mediadores y las pruebas.
		\begin{itemize}
		\item \textbf{prediccion}: contiene el código necesario para la prediccion: \textit{VGGish} (\textit{VGGish2Keras}) y \texttt{FachadaPrediccion}.
		\end{itemize}
	\end{itemize}
	\item \textbf{Disvoice}: Contiene el proyecto \textit{Disvoice} modificado por nosotros. Contiene todos los \textit{scripts} necesarios para la extracción de características con esta herramienta. No analizaremos los subpaquetes en profundidad, ya que no es una herramienta propia. Sin embargo caben destacar 3 paquetes: \texttt{articulacion}, \texttt{phonation} y \texttt{prosody}. Estos contienen los \textit{scripts} para extraer los 3 diferentes tipos de características.
	\item \textbf{VGGish}: Contiene los archivos del repositorio \myurl{https://github.com/tensorflow/models/tree/master/research/audioset}{VGGish} y del repositorio \myurl{https://github.com/antoinemrcr/vggish2Keras}{VGGish2Keras}. También contiene el archivo resultante de la conversión del modelo \textit{VGGish} a \textit{Keras}: \texttt{vggish\_weights.ckpt}. Por último, contiene el \textit{notebook} de extracción de características de VGGish.
\end{itemize}

\imagen{diag_paquetes}{Diagrama de paquetes del proyecto}



