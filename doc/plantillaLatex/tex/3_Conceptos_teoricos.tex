\capitulo{3}{Conceptos teóricos}

Este proyecto abordará diferentes temas dentro del campo conocido como \english{Data Mining}. A su vez se tratan temas sobre el análisis del discurso desde una perspectiva física. Se explicarán conceptos dentro de ambos campos para la correcta comprensión del proyecto.


\section{Minería de datos y aprendizaje automático}

La \concept{minería de datos} es un campo del conocimiento dentro de las ciencias de computación cuyo objetivo general es analizar grandes volúmenes de datos para extraer conocimiento de ellos. Se basa en el concepto de que la sociedad, sobre todo actualmente, produce una gran cantidad de datos y de diferente origen. Sin embargo, extraer conocimiento de los datos no es un proceso tan trivial. No es posible sacar la información de los datos en crudo. Por ello se necesitan diferentes métodos y técnicas que nos permitan hacer esa tarea, como son las técnicas de \concept{aprendizaje automático} \cite{datamining}. Este proceso de divide en varias fases.

\imagen{FasesDM}{Fases en la extracción de conocimiento de un conjunto de datos. \cite{DMprocess}}

Podemos hablar de que la minería de datos es el análisis de un conjunto grande de datos para la obtención de conocimiento de ellos, ya sea relaciones entre esos datos o patrones ocultos, que nos permita obtener una ventaja competitiva o un conocimiento novedoso. Esto está relacionado con el proyecto en cuanto a que queremos obtener conocimiento de la relación voz-enfermedad del Parkinson a través un gran conjunto de audios. A continuación, definiremos varios conceptos dentro de este campo que son manejados en el proyecto.

\subsection{Pre-procesamiento de datos}

Como hemos comentado anteriormente, los datos no pueden ser tratados de manera directa, necesitan ser tratados en una etapa anterior al descubrimiento de información: pre-procesamiento. Esto es indispensable en la fase de pre-procesamiento, transformar los datos en bruto a otro conjunto de datos que pueda servir para ser procesado por los algoritmos necesarios. Específicamente en nuestro caso, no podemos obtener conocimiento de el conjunto de audios en bruto, estos audios deberán ser tratados para extraer un conjunto de características numéricas de cada uno de ellos y poder seguir avanzando a partir de ese punto. El conjunto de características extraídas de los audios será en su mayoría propiedades físicas (\ref{sec:ccas}) o características extraídas con bibliotecas de \english{Deep Learning}. Como nuestro proyecto se basará en aprendizaje supervisado, también se llevará a cabo en esta etapa el etiquetado de los datos.

Otra parte importante, aunque no indispensable, dentro de esta etapa es la fase de selección de características. Cuando tenemos un gran número de atributos para cada instancia puede ser recomendable reducir su dimensionalidad. Una manera de reducir su dimensionalidad es a través de la selección de atributos manual, mediante un análisis estadístico. Un ejemplo de su uso puede verse en \cite{MxLtNovel}, donde varios de ellos son utilizados para definir un subconjunto de 10 características de cada audio dentro de las 132 medidas que se obtenían inicialmente en cada uno de ellos. En nuestro caso, no se realiza ese proceso manualmente. Por un lado, hemos realizado algunos experimentos donde hemos proporcionado todos los atributos de cada audio al algoritmo de clasificación y trasladando a éste la tarea de discernir cuales serán más importantes. Por otro, hemos realizado experimentos donde, en el proceso, se utilizan diferentes métodos selectores de atributos como \english{Select K Best} o \english{Variance Treshold} \textcolor{red}{CITAR SELECTORES, REFERENCIAR A SU APARTADO 3.4} (ver sección \ref{subsec:selectores}). \cite{DMprocess}

\subsection{Aprendizaje automático}
Es el campo de las ciencias de la computación enfocado a que los dispositivos \textit{aprendan} por ellos mismos sin haber sido explícitamente programados para ello. Esta es una rama de la inteligencia artificial. Tras obtener un conjunto válido de características se aplican algoritmos de aprendizaje automático. Estos algoritmos son los encargados de detectar patrones en los datos. Estos algoritmos tienen un fuerte componente matemático y estadístico ya que, a través de métodos de estos campos del conocimientos, se extrae la información de los datos. Se divide en:
\begin{itemize}
	\item \concept{Aprendizaje supervisado}: para cada instancia del conjunto de datos tendremos tanto la entrada como la salida deseada. El objetivo será predecir la salida correcta para una nueva entrada. En nuestro caso la entrada serían las características de los audios y la salida si la persona de ese audio tiene parkinson (clasificación) o en qué nivel lo tiene (regresión) \footnote{Estos ejemplos ilustran la diferencia entre clasificación, donde hay que predecir una etiqueta de entre un conjunto finito de etiquetas posibles, y regresión, donde el valor que se predice es una magnitud continua de un conjunto con un número de valores potencialmente infinito.}.
	\item \concept{Aprendizaje no supervisado}: únicamente se tienen los datos de entrada y no se sabe nada acerca de su salida o clase. Habitualmente estos algoritmos son utilizados para la detección de clases que permitan separar los datos en diferentes grupos. Nosotros no utilizaremos este tipo en nuestro proyecto.
\end{itemize}

\subsubsection{Generalización}
El objetivo de los algoritmos de aprendizaje supervisado no es lograr dar salidas correctas para los ejemplos de datos que tenemos, sino lograr clasificar o dar la salida correcta para un nuevo ejemplo que nos llegue. Se puede decir que el objetivo del aprendizaje automático es en realidad es obtener conocimiento a partir de un gran conjunto de datos el cual sea extrapolable para todos los datos dentro de esa misma situación (de ese mismo problema).
El problema de ajustarse demasiado a los datos de entrenamiento perdiendo así capacidad de generalización se llama sobreajuste. Esto ocurre cuando se crean modelos demasiado complejos que se ajustan a los datos de entrenamiento al 100\%. Al ocurrir sobreajuste, corremos el riesgo de clasificar mal nuevos datos. Si además este sobreajuste a los datos de entrenamiento se combina con la aparición de datos ruidosos en nuestro dataset, se perderá aún más capacidad de generalización. Por ello es importante tener métodos de evaluación de modelos que nos permitan medir cuánto de bueno es nuestro modelo a la hora de generalizar. Aquí aparece el concepto \concept{Cross-Validation}.

\subsubsection{$K$-fold Cross Validation \cite{CrossVal}}
La validación cruzada es un método para la evaluación y comparación de algoritmos. Se basa en descomponer el conjunto de datos en 2 subconjuntos, entrenamiento y test, con el objetivo de evaluar que tal generaliza nuestro modelo. Sin embargo, de esta manera estamos perdiendo información de nuestros datos, ya que el conjunto que utilizamos para test nunca es usado para entrenar. La solución a este problema es la validación cruzada $k$-fold.

En \concept{k-fold cross-validation} el conjunto entero de datos se divide en $k$ conjuntos del mismo tamaño cada uno. A partir de ahí se itera sobre los subconjuntos $k$ veces utilizando cada vez $k-1$ conjuntos para entrenar y 1 conjunto de test. Para cada una de las iteraciones se guarda la performance y finalmente se hace la media para obtener una performance total. El número idóneo de $k$ es 10 según \cite{CrossVal} y así es utilizado el 10-fold en \cite{Orz2016}.

\imagen{3fold_cv}{Ejemplo 3-fold cross-validation extraído de \cite{CrossVal}.}

Cabe destacar que \cite{CrossVal} explica que para \english{k-fold cross-validation} se debe realizar estratificación. \concept{Estratificación} es el proceso por el cual se asegura que una de las $k$ particiones de los datos total es una buena representación de los datos totales. Por ejemplo, en nuestro caso tenemos 50\% de personas con Parkinson y 50\% de personas sanas. Esto significa que para estratificar de manera correcta, cada partición deberá tener la mitad de instancias de cada clase. Cada partición $k$ deberá tener aproximadamente la misma proporción de clases que el conjunto total.

\subsection{Nested Cross Validation}
\textcolor{red}{DEFINIR}.

\subsection{Algoritmos de aprendizaje automático}

A continuación explicaremos varios algoritmos de los utilizados en este proyecto.

\subsubsection{SVM \cite{svmexpl}}

Este algoritmo de aprendizaje supervisado es llamado como máquinas de vector de soporte (SVM). Es un clasificador lineal basado en el concepto de margen máximo. Se utiliza tanto para regresión como para clasificación. Para clasificación su principio fundamental es separar dos clases en el conjunto de datos utilizando un hiperplano. El hiperplano utilizado como separador de las clases será el que maximice el margen entre las clases. Como sabemos, los problemas del mundo real tienen complicaciones como que no sean linealmente separables, que las clases estén solapadas, que haya más de 2 clases de datos, etc. 

Para solucionar el problema de la existencia de más de 2 clases, se utiliza un SVM por cada una de las clases. Sin embargo, para el problemas de separar dos clases que no son linealmente separables, utilizamos el \concept{kernel trick}. Esta característica del algoritmo es importante para poder separar este tipo de datos. Estas funciones kernel son utilizadas para aumentar la dimensionalidad de los datos de entrada, es decir, transformar el espacio de entrada. De esta manera dividiendo linealmente el espacio transformado con el hiperplano pueden resolverse el problema de la separación lineal.  Existen diferentes ejemplos de funciones kernel, entre las más habituales están el kernel, polinomial, funciones de base radial, sigmoide y kernel Gaussiano que es utilizado en \cite{MxLtNovel} y \cite{Orz2016}.

\subsubsection{Random Forest \cite{breiman2001random}}

Es un método de \concept{ensemble} (métodos combinados) basado en los árboles de predicción, la combinación de la selección aleatoria de atributos y el \english{bagging}. El proceso es el siguiente: se construirán de manera independiente un conjunto de árboles, los cuales serán todos diferentes, debido a que están construidos con \english{bagging} y aleatoriedad de atributos, a la hora de predecir una nueva instancia, cada uno hace su predicción y, teniendo en cuenta la decisión de todos los árboles, llegamos a una decisión final conjunta. \concept{Bagging} consiste en obtener varios subconjuntos de datos a partir de un único conjunto mediante remuestreo con reemplazamiento. Esto se hace escogiendo $N$ elementos del conjunto inicial de manera aleatoria pudiendo coger el mismo ejemplo varias veces. Esto se acopla a la construcción de cada árbol de la siguiente manera:
\begin{enumerate}
	\item A la hora escoger el atributo discriminante de un nodo, el mejor lo elegiremos de entre un subconjunto de todos los atributos. El mejor tamaño de este subconjunto es de tamaño $\sqrt{M}$ o $\log M$, siendo $M$ el número de atributos.
	\item Se realiza ese proceso para cada nodo del árbol.
\end{enumerate}
A la hora de realizar la predicción del error se sigue el método \concept{out of bag}. Consiste en recorrer las instancias de entrenamiento y predecir la clase de cada una únicamente con el conjunto de árboles que no han tenido esa instancia en su conjunto de entrenamiento. Se hace ese proceso para todas las instancias que tenemos y predecimos el error.

\subsection{Algoritmos de selección de atributos} \label{subsec:selectores}
\textcolor{red}{DEFINIR}.

\subsection{Grid Search, selección de parámetros} 
\textcolor{red}{DEFINIR}.

\section{Deep Learning}
\textcolor{red}{Se describirá cuando se vea}.

\section{Conceptos estadísticos}
Se describirán a continuación una serie de conceptos matemático-estadísticos utilizados tanto en la evaluación de clasificadores, como en la obtención de medidas de los audios.
\subsection{ROC}
\textcolor{red}{Se describirá cuando se vea.}

\subsection{Medidas de distribución}
Son diferentes medidas extraídas de cada medida física de los audios. Por ejemplo, de la amplitud de onda de un audio se sacan sus 4 funcionales correspondientes con sus momentos de distribución (media, desviación típica, coeficiente de asimetría, curtosis). Se obtienen debido a que caracterizan una muestra de tal manera que si dos distribuciones tienen los momentos iguales son iguales.
\begin{itemize}
	\item \concept{Media}: Es el resultado de la suma de todos los valores dividida entre el número de ellos. Se corresponde con el momento de la distribución de orden 1 respecto a la origen. $m$.
	\item \concept{Desviación}: Mide la dispersión que tienen unos datos respecto a la media. Cuanto mayor sea este valor significará que los datos se encuentran en un rango amplio respecto a la media, mientras que si su valor en bajo significará que los valores se agrupan en un rango cercano a la media. Se corresponde con el momento de la distribución de orden 2 respecto a la media. $d$.
	\item \concept{Oblicuidad o coeficiente de asimetría}: Mide la mayor o menor simetría de la distribución. Contra mayor sea el coeficiente de oblicuidad, mayor será simetría de los datos respecto a la media. Se corresponde con el momento de la distribución de orden 3 respecto a la media. $sk$. 
	\item \concept{Curtosis}: Mide la mayor o menor concentración de datos alrededor de la media. A un mayor valor de este coeficiente, se entiende que los valores están más agrupados en torno a la media y en valores alejados de ella, dejando los tramos intermedios con menor frecuencia. Se corresponde con el momento de la distribución de orden 4 respecto a la media. $k$.
\end{itemize}
Momento de la distribución de orden \textit{r} respecto a la media:\\
\begin{equation}
m_{r} = \sum_{i=1}^{n} (x_{i} - \overline{x})^{r} P(n_{i})
\end{equation}

\section{UPDRS}
\english{Unified Parkinson's disease rating scale} \cite{updrs}, \concept{UPDRS} es la escala universal con la que se mide el grado de severidad del Parkinson. Fue creada para dar un estándar a la evaluación de áreas específicas de la discapacidad. Esta escala evalúa 6 partes fundamentales: 1-Comportamiento, 2-Actividades de la vida diaria, \textbf{3-Examen motor} \cite{updrs3}; 4-Complicaciones de la terapia, 5-Escala de Hoehn \& Yahr (Severidad) \cite{hoehn1967} y 6-Escala de Schwab y England (vida cotidiana).\\
\subsection{UPDRS-III}
Debido al objetivo de nuestro estudio, nos centraremos en la parte 3: examen motor \cite{updrs3}. Utiliza una escala de 0-4, donde 0 es la ausencia de discapacidad motora y 4 discapacidad motora severa. Esta parte trata diversos temas todos relacionados con la capacidad motora corporal. Mide entre otras cosas elementos como el habla, la expresión facial, el temblor o la agilidad. Todo esto afecta a la voz en cuanto se debe realizar un control correcto de los músculos glotales, labios, faringe y más para conseguir una pronunciación satisfactoria. La discapacidad motora  produce a la hora de hablar volúmenes bajos, discurso monótono, articulación imprecisa \cite{Orz2016}. El englobe de estas capacidades se denomina disartria hipocinética \cite{disartia}.

\subsection{Hoehn \& Yahr}
La escala de Hoehn \& Yahr \cite{hoehn1967} evalúa la enfermedad del Parkinson en una escala del 1 al 5. La enfermedad es más severa en función del aumento de la escala. El grado mínimo 1 se corresponde con que la enfermedad es exclusivamente unilateral, aumentando hasta el grado 5 en el que el paciente está en silla de ruedas o en cama, si no tiene ayuda.

\section{Análisis del discurso}
El análisis del discurso se realiza desde 3 prismas diferentes, que son la fonación, la articulación y la prosodia \cite{speechAnalysis}. En nuestro proyecto, fijaremos estos grupos de características a extraer de cada tipo de audios.

\subsection{Análisis de la fonación}
La fonación aborda la vibración de las cuerdas vocales a la hora producir un sonido \cite{speechAnalysis}. Desde el punto de vista clínico, este análisis está relacionado con la curvatura y el cierre incompleto de las cuerdas vocales a la hora de emitir un sonido \cite{phonationfeat}. Las medidas más típicas para el análisis de fonación son Jitter, Shimmer, APQ y PPQ \cite{neurospeech}. Explicadas en la sección \ref{sec:ccas}.

\subsection{Análisis de la articulación}
La articulación comprende la modificación de la posición, el estrés y la forma de los órganos y tejidos involucrados en la producción del habla \cite{speechAnalysis}. Esto se manifiesta en déficits como, por ejemplo, una reducción de la amplitud y la velocidad de los movimientos articulatorios de los labios, la mandíbula y la lengua \cite{articulationfeat}. La capacidad de articulación es evaluada a través de la energía que se libera en las transiciones entre segmentos \textit{voiced} $\rightarrow$ \textit{unvoiced} (transición entre segmentos con voz y sin voz) y viceversa con medidas como MFCC o BBE, explicadas en \ref{sec:ccas}. Se basa en que los pacientes de PD tienen dificultad para comenzar y para detener la vibración de las cuerdas vocales \cite{neurospeech}.

\subsection{Análisis de la prosodia}
La prosodia aborda temas como la variación del volumen, el tono y la sincronización para hablar de manera natural \cite{speechAnalysis}. Se manifiesta con monotonía en el volumen y en el tono, cambios de rapidez en el habla y dificultades a la hora de expresar emociones a través del discurso \cite{emotionfeat}. Las medidas más típicas para el análisis de prosodia son las relacionadas con la frecuencia fundamental, el contorno de la energía y la duración. Explicadas en la sección \ref{sec:ccas}.

\section{Características Físicas de la voz} \label{sec:ccas}
Desde el punto de vista ingenieril y relacionado con el aprendizaje automático, todos los análisis anteriores se deben expresar de una manera numérica para poder crear vectores de características de un audio. Por ello, se extraen diferentes características numéricas de los audios que intentan expresar los análisis de fonación, articulación y prosodia de la manera más óptima posible. Otra dificultad importante desde el punto de vista ingenieril es la continuidad de los audios. Los audios cambian de manera continua en el tiempo, lo que complica el proceso de la extracción de características. Por ello, el audio se divide pequeños segmentos de tiempo en los que se calculará las características de manera estática para ese tramo. Las medidas físicas extraídas de los archivos de voz son de gran variedad y por ello en este apartado explicamos las más comunes y más usadas en los diferentes estudios del análisis del discurso. 

\subsection{Jitter}
El concepto \concept{Jitter} es usado para evaluar la variabilidad temporal durante el envío de señales digitales \cite{wiki:jitter}. Concretamente, en el análisis del discurso, esta medida significa la variación temporal de la frecuencia fundamental del discurso \cite{neurospeech}. $N$ es el número de fragmentos, $M_{f}$ la frecuencia máxima y $F_{0}(k)$ la amplitud de ese frame en concreto:
\begin{equation}
\mathit{Jitter}(\%) = \frac{100}{N M_{f}} \sum_{k=1}^{N}|F_{0}(k)-M_{f}|
\end{equation}

\subsection{Shimmer}
El concepto \textbf{Shimmer} es usado, en el análisis del discurso, para medir la variación temporal de la amplitud del discurso \cite{neurospeech}. $N$ es el número de \english{frames}, $M_{a}$ la amplitud máxima y $A(k)$ la amplitud de ese \english{frame} en concreto:
\begin{equation}
\mathit{Shimmer}(\%) = \frac{100}{N M_{a}} \sum_{k=1}^{N}|A(k)-M_{a}|
\end{equation}

\subsection{APQ y PPQ}
\textbf{APQ} mide la variabilidad a largo plazo de la amplitud de la voz. Para calcularla, se suaviza mediante una media móvil de tamaño 11 y se calcula como la diferencia media absoluta entre la amplitud de un \english{frame} y las amplitudes promediadas sobre sus vecinos, dividida por la amplitud media.

La \textbf{PPQ} mide la variabilidad a largo plazo de la frecuencia fundamental y para calcularla se suaviza mediante una media móvil de tamaño 5. Se calcula como la diferencia promedio absoluta entre la frecuencia de cada cuadro y el promedio de sus vecinos, dividida por la frecuencia media. Ambas medidas se calculan de igual manera, APQ relativa a la amplitud y PPQ relativa a la frecuencia \cite{neurospeech}.

\subsection{Mel Frequency Cepstral Coefficients}
Los \textbf{Coeficientes Cepstrales en las Frecuencias de Mel} son 12 coeficientes para la representación del habla basados en la percepción auditiva humana. Con ellos podemos obtener la información más relevante de una porción de audio, obviando partes menos importantes como el ruido. El cálculo de estos coeficientes está basado en la transformada de Fourier y la transformada del coseno discreta \cite{wiki:mfcc}. Como hemos comentado anteriormente, nos sirve para medir la energía que se libera en las transiciones entre segmentos \textit{voiced} $\rightarrow$ \textit{unvoiced} y viceversa. Tiene un crecimiento logarítmico y se calcula según la siguiente función:
\begin{equation}
m = 1127,01048\ln{\frac{1+f}{700}}
\end{equation}

\subsection{Bark Band Scale}
\textbf{Bark Band Scale} es otro método de caracterización de la energía liberada en las transiciones de la voz. Da 24 coeficientes acorde a la escala de Bark \cite{bbe}. Es una escala psicoacústica en la que se definen límites de frecuencias para definir la escala (i.e. grado 1 [100Hz-200Hz], grado 2 [200Hz-300Hz],..., grado 23 [12000-15500], grado 24 [15500- $\infty$])
Tiene un crecimiento logarítmico y se calcula según la siguiente función:
\begin{equation}
\mathit{Bark}(f) = 13 \arctan(0.00076 f)+ 3.5 \arctan \left ( \frac{f}{7500} \right )^{2}
\end{equation}


\subsection{Medidas relacionadas con la frecuencia fundamental}
Su objetivo es caracterizar los patrones de entonación de la voz. Por ello, se calcula el contorno de la frecuencia máxima y se calculan diferentes estadísticos sobre ella, como medias en Hz, desviación en Hz, máximos, etc \cite{neurospeech}.

\subsection{Medidas relacionadas con la energía}
Al igual que para el anterior caso, pero esta vez con la energía. Se calculan medidas estadísticas sobre el contorno de la energía: media en dB, desviación en dB, máximo o el coeficiente de regresión entre el contorno de energía y una regresión lineal \cite{neurospeech}.
